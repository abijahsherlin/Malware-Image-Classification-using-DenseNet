#importing the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
import itertools
from sklearn.model_selection import train_test_split
import tensorflow
from keras.models import Model
from keras import models
from keras import layers
from keras import backend as K
from keras.metrics import top_k_categorical_accuracy
from keras.applications.densenet import DenseNet201, preprocess_input
from keras.models import model_from_json
from keras.layers import *
from keras.models import Sequential, load_model
from keras.optimizers import SGD,Adam, RMSprop
from keras.callbacks import ReduceLROnPlateau
from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils.multiclass import unique_labels
import glob
import h5py
import cv2
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array, load_img
import time
from time import sleep
import datetime
import os
import json
from imutils import paths
from sklearn.preprocessing import label_binarize

#Initialize all necessary variables
weights = 'imagenet'
include_top = False
image_size=(64,64,3) 
BS= 32
n_epochs= 50
num_class = 9
INIT_LR=0.0001
opt = Adam(lr=INIT_LR)
train_path="dataset location"
seed_val=np.random.seed(9)
test_size = 0.30
results = '/.../results.txt'
features_path='/.../features.hdf5'
labels_path='/.../labels.hdf5'


base_model = DenseNet201(include_top=include_top,weights=weights,input_tensor=Input(shape=(image_size)),pooling='avg')
base_model.summary()

top=base_model.output
# Add new layers
#top=Flatten(name="flatten")(top)
print(top)
top=Dense(num_class, activation='softmax')(top)
print(top)
# Create the model
model = Model(inputs=base_model.input, outputs=top)
print(model)
model.summary()
# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = True
# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)

train_labels = sorted(os.listdir(train_path))

data = []
labels = []
print("[INFO] loading images...")
imagePaths = (list(paths.list_images(train_path)))

i = 0
for label in train_labels:
  cur_path = train_path + "/" + label
  for image_path in glob.glob(cur_path + "/*"):
    img = load_img(image_path, target_size=(64,64))  ################ Image size change ########################
    #print(img)
    x = img_to_array(img)
    x = preprocess_input(x)
    #print("[INFO]img_to_array - ",x.shape)
    data.append(x) #add feature to end of list
    labels.append(label)
    #print ("[INFO] processed - {}".format(i))
    i += 1
  print ("[INFO] completed label - {}".format(label))

data =np.array(data)
#img_data = img_data.astype('float32')
print (data.shape)


# encode the labels using LabelEncoder
targetNames = np.unique(labels) #to extract unique labels
le = LabelEncoder()
le_labels = le.fit_transform(labels)
le_labels=label_binarize(le_labels,[0,1,2,3,4,5,6,7,8,9])
# save features and labels 
h5f_data = h5py.File(features_path, 'w')
h5f_data.create_dataset('dataset_1', data=data)

h5f_label = h5py.File(labels_path, 'w')
h5f_label.create_dataset('dataset_1', data=le_labels)
h5f_data.close()
h5f_label.close()

print ("[STATUS] features and labels saved..")
# load the bottleneck features saved earlier
print ("[STATUS] training labels: {}".format(le_labels))
print ("[STATUS] training labels shape: {}".format(le_labels.shape))
# partition the data into training and testing splits using 70% of
# the data for training and the remaining 30% for testing
(trainX, testX, trainY, testY) = train_test_split(data,
	le_labels, test_size=test_size, random_state=seed_val)

print(trainX.shape)
print(testX.shape)
print(trainY.shape)
print(testY.shape)

#Model summary
model.summary()

#Compiling the model
top5 = top_k_categorical_accuracy
model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy', top5])

#Training the model
fit=model.fit(trainX, trainY, epochs = n_epochs, batch_size = BS, validation_data=(testX, testY))


model.save_weights('/.../model.h5') 
#Test the data
preds=model.predict(testX)

#Plotting the training accuracy and validation accuracy
plt.plot(fit.history['accuracy'],color='b')
plt.plot(fit.history['val_accuracy'],'--',color='r')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# setting x and y axis range 
plt.ylim(0,1) 
#plt.xlim(0,1) 
plt.legend(['Train', 'Test'], loc='upper right')


#Assigning the first subplot to graph training loss and validation loss
plt.plot(fit.history['loss'],color='b')
plt.plot(fit.history['val_loss'],'--',color='r')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper right')


scores = model.evaluate(testX, testY)

# use rank-1 and rank-5 predictions
print ("[INFO] evaluating model...")
f = open(results, "w")
print ("top 1 Accuracy: %.2f%%" %(scores[1]*100))
print("top 5 Accuracy: %.2f%%"%(scores[2]*100))
print(classification_report(testY.argmax(axis=1), preds.argmax(axis=1)))

f.write("Model took %0.2f seconds to preprocess\n"%(t1))
f.write("Model took %0.2f seconds to train\n"%(t2))
f.write("Model took %0.2f seconds to preprocess and train\n"%(t3))
f.write("Test Accuracy: %.2f%%\n" %(scores[1]*100))
f.write("Test loss: %.2f\n" %(scores[0]))
f.write("Model took %0.2f seconds to test\n"%(t4))
f.write("top 5 Accuracy: {:.2f}%\n\n".format(scores[2]*100))

# write the classification report to file
f.write("{}\n".format(classification_report(testY.argmax(axis=1), preds.argmax(axis=1))))
f.close()



